{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11722918,"sourceType":"datasetVersion","datasetId":7359046}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tqdm opencv-python==4.5.2.52 opencv-python-headless==4.5.2.52","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:22:46.697098Z","iopub.execute_input":"2025-06-05T15:22:46.697363Z","iopub.status.idle":"2025-06-05T15:22:49.240044Z","shell.execute_reply.started":"2025-06-05T15:22:46.697335Z","shell.execute_reply":"2025-06-05T15:22:49.239123Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n\u001b[31mERROR: Ignored the following yanked versions: 3.4.11.39, 3.4.17.61, 4.4.0.42, 4.4.0.44, 4.5.4.58, 4.5.5.62, 4.7.0.68\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python==4.5.2.52 (from versions: 3.4.0.14, 3.4.10.37, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.60, 4.5.5.64, 4.6.0.66, 4.7.0.72, 4.8.0.74, 4.8.0.76, 4.8.1.78, 4.9.0.80, 4.10.0.82, 4.10.0.84, 4.11.0.86)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python==4.5.2.52\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install remotezip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:22:49.241488Z","iopub.execute_input":"2025-06-05T15:22:49.241828Z","iopub.status.idle":"2025-06-05T15:22:52.356984Z","shell.execute_reply.started":"2025-06-05T15:22:49.241801Z","shell.execute_reply":"2025-06-05T15:22:52.356085Z"}},"outputs":[{"name":"stdout","text":"Collecting remotezip\n  Downloading remotezip-0.12.3-py3-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from remotezip) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->remotezip) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->remotezip) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->remotezip) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->remotezip) (2025.4.26)\nDownloading remotezip-0.12.3-py3-none-any.whl (8.1 kB)\nInstalling collected packages: remotezip\nSuccessfully installed remotezip-0.12.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install tf-models-official","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:22:52.358053Z","iopub.execute_input":"2025-06-05T15:22:52.358281Z","iopub.status.idle":"2025-06-05T15:24:08.529509Z","shell.execute_reply.started":"2025-06-05T15:22:52.358255Z","shell.execute_reply":"2025-06-05T15:24:08.528719Z"}},"outputs":[{"name":"stdout","text":"Collecting tf-models-official\n  Downloading tf_models_official-2.19.1-py2.py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (1.17.0)\nRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (2.164.0)\nRequirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (1.7.4.2)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (1.26.4)\nRequirement already satisfied: oauth2client in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (4.1.3)\nRequirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (2.2.3)\nRequirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (7.0.0)\nRequirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (9.0.0)\nRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (1.15.2)\nRequirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (0.16.1)\nCollecting tensorflow-model-optimization>=0.4.1 (from tf-models-official)\n  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\nRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (4.9.8)\nRequirement already satisfied: tf-keras>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (2.18.0)\nRequirement already satisfied: gin-config in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (0.5.0)\nRequirement already satisfied: tf_slim>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (1.1.0)\nRequirement already satisfied: Cython in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (3.0.12)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (3.7.2)\nRequirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (6.0.2)\nRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (4.11.0.86)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (11.1.0)\nRequirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (2.0.8)\nCollecting seqeval (from tf-models-official)\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (0.2.0)\nCollecting sacrebleu (from tf-models-official)\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (4.2.1)\nCollecting ai-edge-litert>=1.0.1 (from tf-models-official)\n  Downloading ai_edge_litert-1.3.0-cp311-cp311-manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting tensorflow~=2.19.0 (from tf-models-official)\n  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\nCollecting tensorflow-text~=2.19.0 (from tf-models-official)\n  Downloading tensorflow_text-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\nCollecting backports.strenum (from ai-edge-litert>=1.0.1->tf-models-official)\n  Downloading backports_strenum-1.2.8-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from ai-edge-litert>=1.0.1->tf-models-official) (25.2.10)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from ai-edge-litert>=1.0.1->tf-models-official) (4.67.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from ai-edge-litert>=1.0.1->tf-models-official) (4.13.2)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.22.0)\nRequirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.40.1)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.2.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.34.1)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (4.1.1)\nRequirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (6.2.0)\nRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (2025.4.26)\nRequirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (3.4.2)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (3.10)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (3.20.3)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.9.0.post0)\nRequirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (8.0.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.32.3)\nRequirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (75.2.0)\nRequirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (1.3)\nRequirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.4.0)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (0.5.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->tf-models-official) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->tf-models-official) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->tf-models-official) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->tf-models-official) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->tf-models-official) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->tf-models-official) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.22.0->tf-models-official) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.22.0->tf-models-official) (2025.2)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.19.0->tf-models-official) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.19.0->tf-models-official) (1.6.3)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.19.0->tf-models-official) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.19.0->tf-models-official) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.19.0->tf-models-official) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.19.0->tf-models-official) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.19.0->tf-models-official) (25.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.19.0->tf-models-official) (3.0.1)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.19.0->tf-models-official) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.19.0->tf-models-official) (1.72.0rc1)\nCollecting tensorboard~=2.19.0 (from tensorflow~=2.19.0->tf-models-official)\n  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.19.0->tf-models-official) (3.8.0)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.19.0->tf-models-official) (3.13.0)\nCollecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow~=2.19.0->tf-models-official)\n  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.19.0->tf-models-official) (0.37.1)\nRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.9)\nINFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\nCollecting tf-keras>=2.16.0 (from tf-models-official)\n  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->tf-models-official) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->tf-models-official) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->tf-models-official) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->tf-models-official) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->tf-models-official) (3.0.9)\nRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client->tf-models-official) (0.6.1)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from oauth2client->tf-models-official) (0.4.2)\nRequirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from oauth2client->tf-models-official) (4.9.1)\nCollecting portalocker (from sacrebleu->tf-models-official)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu->tf-models-official) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu->tf-models-official) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu->tf-models-official) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu->tf-models-official) (5.3.1)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval->tf-models-official) (1.2.2)\nRequirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->tf-models-official) (0.7.1)\nRequirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->tf-models-official) (1.12.2)\nRequirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->tf-models-official) (19.0.1)\nRequirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->tf-models-official) (0.1.7)\nRequirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->tf-models-official) (1.17.0)\nRequirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->tf-models-official) (0.10.2)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow~=2.19.0->tf-models-official) (0.45.1)\nRequirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow-model-optimization>=0.4.1->tf-models-official) (25.3.0)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->tf-models-official) (0.8.1)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->tf-models-official) (2025.3.2)\nRequirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->tf-models-official) (6.5.2)\nRequirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->tf-models-official) (3.21.0)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official) (1.70.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official) (5.5.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow~=2.19.0->tf-models-official) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow~=2.19.0->tf-models-official) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow~=2.19.0->tf-models-official) (0.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.6.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow~=2.19.0->tf-models-official) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow~=2.19.0->tf-models-official) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow~=2.19.0->tf-models-official) (3.1.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->tf-models-official) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->tf-models-official) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20->tf-models-official) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20->tf-models-official) (2024.2.0)\nRequirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow-datasets->tf-models-official) (0.16)\nINFO: pip is looking at multiple versions of tensorflow-metadata to determine which version is compatible with other requirements. This could take a while.\nCollecting tensorflow-metadata (from tensorflow-datasets->tf-models-official)\n  Downloading tensorflow_metadata-1.17.1-py3-none-any.whl.metadata (2.6 kB)\n  Downloading tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n  Downloading tensorflow_metadata-1.16.0-py3-none-any.whl.metadata (2.4 kB)\n  Downloading tensorflow_metadata-1.15.0-py3-none-any.whl.metadata (2.4 kB)\n  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20->tf-models-official) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow~=2.19.0->tf-models-official) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow~=2.19.0->tf-models-official) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow~=2.19.0->tf-models-official) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow~=2.19.0->tf-models-official) (0.1.2)\nDownloading tf_models_official-2.19.1-py2.py3-none-any.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading ai_edge_litert-1.3.0-cp311-cp311-manylinux_2_17_x86_64.whl (11.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorflow_text-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading backports_strenum-1.2.8-py3-none-any.whl (7.9 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nDownloading tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=428ab120d23b9b2f02127236739ba69c9993747cd31c73135514577cd770f4b1\n  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\nSuccessfully built seqeval\nInstalling collected packages: portalocker, backports.strenum, tensorflow-metadata, ml-dtypes, tensorboard, tensorflow, tf-keras, tensorflow-text, tensorflow-model-optimization, seqeval, sacrebleu, ai-edge-litert, tf-models-official\n  Attempting uninstall: tensorflow-metadata\n    Found existing installation: tensorflow-metadata 1.17.0\n    Uninstalling tensorflow-metadata-1.17.0:\n      Successfully uninstalled tensorflow-metadata-1.17.0\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.4.1\n    Uninstalling ml-dtypes-0.4.1:\n      Successfully uninstalled ml-dtypes-0.4.1\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.18.0\n    Uninstalling tensorboard-2.18.0:\n      Successfully uninstalled tensorboard-2.18.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.18.0\n    Uninstalling tensorflow-2.18.0:\n      Successfully uninstalled tensorflow-2.18.0\n  Attempting uninstall: tf-keras\n    Found existing installation: tf_keras 2.18.0\n    Uninstalling tf_keras-2.18.0:\n      Successfully uninstalled tf_keras-2.18.0\n  Attempting uninstall: tensorflow-text\n    Found existing installation: tensorflow-text 2.18.1\n    Uninstalling tensorflow-text-2.18.1:\n      Successfully uninstalled tensorflow-text-2.18.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ai-edge-litert-1.3.0 backports.strenum-1.2.8 ml-dtypes-0.5.1 portalocker-3.1.1 sacrebleu-2.5.1 seqeval-1.2.2 tensorboard-2.19.0 tensorflow-2.19.0 tensorflow-metadata-1.14.0 tensorflow-model-optimization-0.8.0 tensorflow-text-2.19.0 tf-keras-2.19.0 tf-models-official-2.19.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import tqdm\nimport random\nimport pathlib\nimport itertools\nimport collections\n\nimport cv2\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport keras\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\n\n# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\nfrom official.projects.movinet.modeling import movinet\nfrom official.projects.movinet.modeling import movinet_model\n\nimport remotezip as rz\nfrom collections import defaultdict\nimport pandas as pd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:24:08.531707Z","iopub.execute_input":"2025-06-05T15:24:08.532005Z","iopub.status.idle":"2025-06-05T15:24:17.763012Z","shell.execute_reply.started":"2025-06-05T15:24:08.531983Z","shell.execute_reply":"2025-06-05T15:24:17.762471Z"}},"outputs":[{"name":"stderr","text":"2025-06-05 15:24:10.195899: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749137050.217997      35 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749137050.224705      35 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1749137050.242786      35 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1749137050.242808      35 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1749137050.242811      35 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1749137050.242813      35 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Load the CSV\nlabels_df = pd.read_csv('/kaggle/input/changing-tires/labels.csv')\n\naction_to_label = {}\nfor path in labels_df['video_path']:\n    folder = path.split('/')[0]\n    label = labels_df[labels_df['video_path'].str.startswith(folder)].iloc[0]['label']\n    action_to_label[folder] = label\n\nvideo_counts = defaultdict(int)\nfor path in labels_df['video_path']:\n    folder = path.split('/')[0]\n    video_counts[folder] += 1\n\n\nprint(\"Action to Label Mapping:\")\nfor action, label in action_to_label.items():\n    print(f\"{action}: {label}\")\n\nprint(\"=\" * 40)\n\nprint(\"Video Counts per Action:\")\nfor action, count in video_counts.items():\n    print(f\"{action}: {count} videos\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:24:17.763683Z","iopub.execute_input":"2025-06-05T15:24:17.764129Z","iopub.status.idle":"2025-06-05T15:24:18.235851Z","shell.execute_reply.started":"2025-06-05T15:24:17.764110Z","shell.execute_reply":"2025-06-05T15:24:18.235154Z"}},"outputs":[{"name":"stdout","text":"Action to Label Mapping:\nhand_tighten_bolts: 0\ninitial_wrench_tighten: 1\nlift_car_with_jack: 2\nloosen_bolts: 3\nlower_car: 4\nplace_spare_tire: 5\nremove_bolts: 6\nremove_tire: 7\ntighten_bolts: 8\n========================================\nVideo Counts per Action:\nhand_tighten_bolts: 100 videos\ninitial_wrench_tighten: 100 videos\nlift_car_with_jack: 100 videos\nloosen_bolts: 100 videos\nlower_car: 100 videos\nplace_spare_tire: 100 videos\nremove_bolts: 100 videos\nremove_tire: 77 videos\ntighten_bolts: 100 videos\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import tensorflow as tf\nimport random\nimport numpy as np\nimport cv2\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport os # Added for os.system\n\n# Official TensorFlow Model Garden imports for MoViNet\n# Ensure this library is installed: pip install tf-models-official\nfrom official.projects.movinet.modeling import movinet\nfrom official.projects.movinet.modeling import movinet_model\n\n# Frame processing functions\ndef format_frames(frame, output_size=(224, 224)):\n    \"\"\"\n    Pad and resize an image from a video.\n    \n    Args:\n      frame: Image that needs to resized and padded. \n      output_size: Pixel size of the output frame image.\n\n    Return:\n      Formatted frame with padding of specified output size.\n    \"\"\"\n    frame = tf.image.convert_image_dtype(frame, tf.float32)\n    frame = tf.image.resize_with_pad(frame, *output_size)\n    return frame\n\ndef frames_from_video_file(video_path, n_frames, output_size=(224, 224), frame_step=15):\n    \"\"\"\n    Creates frames from each video file present for each category.\n\n    Args:\n      video_path: File path to the video.\n      n_frames: Number of frames to be created per video file.\n      output_size: Pixel size of the output frame image.\n      frame_step: Number of frames to step over after each selected frame.\n\n    Return:\n      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n    \"\"\"\n    result = []\n    src = cv2.VideoCapture(str(video_path)) \n\n    video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n    need_length = 1 + (n_frames - 1) * frame_step\n\n    if need_length > video_length:\n        start = 0\n    else:\n        max_start = video_length - need_length\n        # Ensure max_start is non-negative before passing to randint\n        start = random.randint(0, int(max(0, max_start)))\n\n\n    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n    ret, frame = src.read()\n    if ret:\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        result.append(format_frames(frame_rgb, output_size))\n    else:\n        print(f\"Warning: Could not read first frame from {video_path}. Appending zeros.\")\n        result.append(tf.zeros((*output_size, 3), dtype=tf.float32))\n\n    for _ in range(n_frames - 1):\n        for _ in range(frame_step):\n            ret, frame = src.read()\n        if ret:\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame_formatted = format_frames(frame_rgb, output_size)\n            result.append(frame_formatted)\n        else:\n            if len(result) > 0:\n                 result.append(tf.zeros_like(result[0]))\n            else:\n                 result.append(tf.zeros((*output_size, 3), dtype=tf.float32))\n    src.release()\n    \n    if not all(isinstance(r, tf.Tensor) for r in result):\n        print(\"Warning: Not all frames are tensors. This might indicate issues in frame extraction.\")\n        result = [tf.convert_to_tensor(r, dtype=tf.float32) if not isinstance(r, tf.Tensor) else r for r in result]\n\n    if result and len(result) == n_frames:\n        try:\n            result_array = tf.stack(result, axis=0).numpy()\n        except tf.errors.InvalidArgumentError as e:\n            print(f\"Error stacking frames for {video_path}: {e}. Shape of results: {[r.shape for r in result]}\")\n            return np.zeros((n_frames, *output_size, 3), dtype=np.float32)\n    elif not result:\n         print(f\"Warning: No frames extracted for {video_path}. Returning zeros.\")\n         return np.zeros((n_frames, *output_size, 3), dtype=np.float32)\n    else: \n         print(f\"Warning: Extracted {len(result)} frames, expected {n_frames} for {video_path}. Returning zeros.\")\n         return np.zeros((n_frames, *output_size, 3), dtype=np.float32)\n    return result_array\n\nclass FrameGenerator:\n    def __init__(self, data_dir, split_df, n_frames, training=False, class_names_list=None):\n        self.data_dir = Path(data_dir)\n        self.split_df = split_df \n        self.n_frames = n_frames\n        self.training = training\n        \n        self.video_paths = [self.data_dir / path for path in self.split_df['video_path']]\n        self.labels = self.split_df['label'].tolist()\n        \n        if class_names_list:\n            self.class_names = class_names_list\n        else:\n            unique_sorted_labels = sorted(self.split_df['label'].unique())\n            self.class_names = [f\"class_{i}\" for i in unique_sorted_labels]\n\n    def __call__(self):\n        pairs = list(zip(self.video_paths, self.labels))\n        if self.training:\n            random.shuffle(pairs)\n        \n        for path, label in pairs:\n            # Pass resolution to frames_from_video_file\n            video_frames = frames_from_video_file(path, self.n_frames, (resolution, resolution))\n            yield video_frames, tf.cast(label, tf.int16)\n\n# --- Main script starts ---\ndata_dir = Path(\"/kaggle/input/changing-tires\") \nn_frames = 10 \nresolution = 224 \n\ntry:\n    df = pd.read_csv(data_dir / \"labels.csv\")\nexcept FileNotFoundError:\n    print(f\"Error: labels.csv not found at {data_dir / 'labels.csv'}\")\n    df = pd.DataFrame({\n        'video_path': [f'dummy_class/video{i}.mp4' for i in range(10)],\n        'label': [i % 2 for i in range(10)] \n    })\n    # Create dummy video files if using dummy data for testing locally\n    # For kaggle, this part might not be needed if using actual data\n    if str(data_dir) == \".\": # If using local dummy data\n        for i in range(10):\n            dummy_video_path = Path(f\"dummy_class/video{i}.mp4\")\n            dummy_video_path.parent.mkdir(parents=True, exist_ok=True)\n            # Create a very short, tiny dummy mp4 if opencv can handle it\n            # Otherwise, this part needs actual dummy video files.\n            # For now, just creating empty files for path existence.\n            if not dummy_video_path.exists():\n                 # Creating an empty file, opencv will fail to read but path exists\n                with open(dummy_video_path, 'w') as fp:\n                    pass # create empty file\n        print(\"Created dummy video file paths for local testing.\")\n\n\nnum_classes = df['label'].nunique()\nprint(f\"Number of classes in the dataset: {num_classes}\")\n\nunique_sorted_labels = sorted(df['label'].unique())\nclass_names_list = [f\"action_{label_val}\" for label_val in unique_sorted_labels]\n\n\ntrain_df, temp_df = train_test_split(\n    df, \n    test_size=0.3, \n    stratify=df['label'] if num_classes > 1 and len(df) >= num_classes * 2 else None,\n    random_state=42\n)\n\nval_df, test_df = train_test_split(\n    temp_df,\n    test_size=0.5, \n    stratify=temp_df['label'] if num_classes > 1 and len(temp_df) >= num_classes * 2 else None,\n    random_state=42\n)\n\n# Corrected percentage calculation and formatting\ntrain_percentage = (len(train_df) / len(df) * 100) if len(df) > 0 else 0.0\nval_percentage = (len(val_df) / len(df) * 100) if len(df) > 0 else 0.0\ntest_percentage = (len(test_df) / len(df) * 100) if len(df) > 0 else 0.0\n\nprint(f\"Train set: {len(train_df)} samples ({train_percentage:.1f}%)\")\nprint(f\"Validation set: {len(val_df)} samples ({val_percentage:.1f}%)\")\nprint(f\"Test set: {len(test_df)} samples ({test_percentage:.1f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:24:18.236671Z","iopub.execute_input":"2025-06-05T15:24:18.236877Z","iopub.status.idle":"2025-06-05T15:24:18.374021Z","shell.execute_reply.started":"2025-06-05T15:24:18.236854Z","shell.execute_reply":"2025-06-05T15:24:18.373412Z"}},"outputs":[{"name":"stdout","text":"Number of classes in the dataset: 9\nTrain set: 613 samples (69.9%)\nValidation set: 132 samples (15.1%)\nTest set: 132 samples (15.1%)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"train_generator = FrameGenerator(data_dir, train_df, n_frames, training=True, class_names_list=class_names_list)\nval_generator = FrameGenerator(data_dir, val_df, n_frames, training=False, class_names_list=class_names_list)\ntest_generator = FrameGenerator(data_dir, test_df, n_frames, training=False, class_names_list=class_names_list)\n\noutput_signature = (\n    tf.TensorSpec(shape=(n_frames, resolution, resolution, 3), dtype=tf.float32),\n    tf.TensorSpec(shape=(), dtype=tf.int16)\n)\n\ntrain_ds = tf.data.Dataset.from_generator(train_generator, output_signature=output_signature).cache().prefetch(tf.data.AUTOTUNE)\nval_ds = tf.data.Dataset.from_generator(val_generator, output_signature=output_signature).cache().prefetch(tf.data.AUTOTUNE)\ntest_ds = tf.data.Dataset.from_generator(test_generator, output_signature=output_signature).cache().prefetch(tf.data.AUTOTUNE)\n\nbatch_size = 8 \ntrain_ds = train_ds.batch(batch_size)\nval_ds = val_ds.batch(batch_size)\ntest_ds = test_ds.batch(batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:24:18.374742Z","iopub.execute_input":"2025-06-05T15:24:18.375443Z","iopub.status.idle":"2025-06-05T15:24:19.574819Z","shell.execute_reply.started":"2025-06-05T15:24:18.375417Z","shell.execute_reply":"2025-06-05T15:24:19.574127Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1749137059.449912      35 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"model_id = 'a0'\ntf.keras.backend.clear_session()\n\ncheckpoint_dir_name = f'movinet_{model_id}_base'\ncheckpoint_dir_path = Path(checkpoint_dir_name)\n\nif not checkpoint_dir_path.exists():\n    print(f\"Downloading and extracting {model_id} weights...\")\n    wget_command = f\"wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_{model_id}_base.tar.gz -O movinet_{model_id}_base.tar.gz -q\"\n    tar_command = f\"tar -xvf movinet_{model_id}_base.tar.gz\"\n    if os.system(wget_command) == 0:\n        os.system(tar_command)\n    else:\n        print(f\"Failed to download weights. Ensure wget and tar are installed or download manually.\")\n        if not checkpoint_dir_path.exists(): print(f\"Checkpoint dir {checkpoint_dir_path} still not found.\")\nelse:\n    print(f\"Checkpoint directory {checkpoint_dir_path} already exists.\")\n\ncheckpoint_path = tf.train.latest_checkpoint(checkpoint_dir_path)\nif not checkpoint_path: print(f\"Error: No checkpoint in {checkpoint_dir_path}.\")\nelse: print(f\"Found checkpoint: {checkpoint_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:24:19.575524Z","iopub.execute_input":"2025-06-05T15:24:19.575756Z","iopub.status.idle":"2025-06-05T15:24:20.096241Z","shell.execute_reply.started":"2025-06-05T15:24:19.575738Z","shell.execute_reply":"2025-06-05T15:24:20.095521Z"}},"outputs":[{"name":"stdout","text":"Downloading and extracting a0 weights...\nmovinet_a0_base/\nmovinet_a0_base/checkpoint\nmovinet_a0_base/ckpt-1.data-00000-of-00001\nmovinet_a0_base/ckpt-1.index\nFound checkpoint: movinet_a0_base/ckpt-1\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 1. Create a temporary non-causal model to load the full checkpoint\ntemp_backbone_non_causal = movinet.Movinet(model_id=model_id, causal=False)\ntemp_model_for_loading = movinet_model.MovinetClassifier(backbone=temp_backbone_non_causal, num_classes=600)\ndummy_input_shape_for_loading = [1, n_frames, resolution, resolution, 3]\ntemp_model_for_loading.build(dummy_input_shape_for_loading)\n\nif checkpoint_path:\n    print(\"Loading pre-trained weights into temporary non-causal model...\")\n    load_checkpoint = tf.train.Checkpoint(model=temp_model_for_loading)\n    status = load_checkpoint.restore(checkpoint_path)\n    try:\n        status.assert_existing_objects_matched()\n        print(\"Successfully loaded pre-trained weights into temporary model.\")\n    except AssertionError as e:\n        print(f\"Warning: Weight loading status: {e}. Some weights might not have matched.\")\n        status.expect_partial()\nelse:\n    print(\"Skipping weight loading as checkpoint path is not available.\")\n\n# 2. Create the backbone for training (causal, but NOT using external states for fit())\ntraining_backbone = movinet.Movinet(\n    model_id=model_id,\n    causal=True,\n    use_external_states=False # Important for training with model.fit()\n)\n\n# 3. Create the model for training\n# output_states=False because standard loss functions don't expect states\nmodel_for_training = movinet_model.MovinetClassifier(\n    backbone=training_backbone,\n    num_classes=num_classes, # Your dataset's number of classes\n    output_states=False      # Important for training with model.fit()\n)\n\n# 4. Build the training model\nmodel_for_training.build([None, n_frames, resolution, resolution, 3])\n\n# 5. Transfer backbone weights from the loaded non-causal backbone to the training backbone\nif checkpoint_path and hasattr(temp_model_for_loading, 'backbone') and hasattr(model_for_training, 'backbone'):\n    print(\"Transferring backbone weights from temporary model to training model...\")\n    try:\n        model_for_training.backbone.set_weights(temp_model_for_loading.backbone.get_weights())\n        print(\"Successfully transferred backbone weights to training model.\")\n    except ValueError as e:\n        print(f\"Error transferring backbone weights to training model: {e}\")\nelse:\n     print(\"Skipping backbone weight transfer to training model.\")\n\n# 6. Set backbone trainability\nmodel_for_training.backbone.trainable = True \n\n# --- Fine-tuning ---\nnum_epochs = 4\nloss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\nmodel_for_training.compile(\n    loss=loss_obj,\n    optimizer='adam',\n    metrics=['accuracy']\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:24:20.097091Z","iopub.execute_input":"2025-06-05T15:24:20.097738Z","iopub.status.idle":"2025-06-05T15:24:42.283042Z","shell.execute_reply.started":"2025-06-05T15:24:20.097715Z","shell.execute_reply":"2025-06-05T15:24:42.282365Z"}},"outputs":[{"name":"stdout","text":"Loading pre-trained weights into temporary non-causal model...\nSuccessfully loaded pre-trained weights into temporary model.\nTransferring backbone weights from temporary model to training model...\nSuccessfully transferred backbone weights to training model.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"history = model_for_training.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=5,\n    verbose=1,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:24:42.285601Z","iopub.execute_input":"2025-06-05T15:24:42.285878Z","iopub.status.idle":"2025-06-05T15:38:08.596130Z","shell.execute_reply.started":"2025-06-05T15:24:42.285858Z","shell.execute_reply":"2025-06-05T15:38:08.595535Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1749137134.842644     126 service.cc:152] XLA service 0x78e814361c40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1749137134.842681     126 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1749137134.870227     126 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1749137135.795285     126 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n2025-06-05 15:25:37.641511: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n","output_type":"stream"},{"name":"stdout","text":"77/77 [==============================] - 720s 8s/step - loss: 1.3019 - accuracy: 0.5334 - val_loss: 1.2904 - val_accuracy: 0.5530\nEpoch 2/5\n77/77 [==============================] - 22s 282ms/step - loss: 0.3840 - accuracy: 0.8695 - val_loss: 1.2906 - val_accuracy: 0.6818\nEpoch 3/5\n77/77 [==============================] - 22s 280ms/step - loss: 0.1596 - accuracy: 0.9543 - val_loss: 1.0502 - val_accuracy: 0.7879\nEpoch 4/5\n77/77 [==============================] - 21s 279ms/step - loss: 0.1445 - accuracy: 0.9511 - val_loss: 0.5813 - val_accuracy: 0.8333\nEpoch 5/5\n77/77 [==============================] - 22s 279ms/step - loss: 0.1497 - accuracy: 0.9560 - val_loss: 0.4868 - val_accuracy: 0.8636\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Loss Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Accuracy Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\nplt.savefig(\"training_history_plots.png\") # Save the plot\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:38:08.596886Z","iopub.execute_input":"2025-06-05T15:38:08.597103Z","iopub.status.idle":"2025-06-05T15:38:08.978062Z","shell.execute_reply.started":"2025-06-05T15:38:08.597086Z","shell.execute_reply":"2025-06-05T15:38:08.977502Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(\"Evaluating on the test set with the training model...\")\nloss, accuracy = model_for_training.evaluate(test_ds, verbose=1)\nprint(f\"Test Loss (training model): {loss:.4f}\")\nprint(f\"Test Accuracy (training model): {accuracy:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:38:08.978785Z","iopub.execute_input":"2025-06-05T15:38:08.978998Z","iopub.status.idle":"2025-06-05T15:39:40.136729Z","shell.execute_reply.started":"2025-06-05T15:38:08.978980Z","shell.execute_reply":"2025-06-05T15:39:40.136099Z"}},"outputs":[{"name":"stdout","text":"Evaluating on the test set with the training model...\n17/17 [==============================] - 91s 5s/step - loss: 0.2863 - accuracy: 0.9318\nTest Loss (training model): 0.2863\nTest Accuracy (training model): 0.9318\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def get_actual_predicted_labels(dataset):\n  \"\"\"\n    Create a list of actual ground truth values and the predictions from the model.\n\n    Args:\n      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n\n    Return:\n      Ground truth and predicted values for a particular dataset.\n  \"\"\"\n  actual = [labels for _, labels in dataset.unbatch()]\n  predicted = model_for_training.predict(dataset)\n\n  actual = tf.stack(actual, axis=0)\n  predicted = tf.concat(predicted, axis=0)\n  predicted = tf.argmax(predicted, axis=1)\n\n  return actual, predicted\n\ndef plot_confusion_matrix(actual, predicted, labels, ds_type, filename=\"confusion_matrix.png\"):\n  \n  cm = tf.math.confusion_matrix(actual, predicted)\n  plt.figure(figsize=(12, 12))\n  ax = sns.heatmap(cm, annot=True, fmt='g')\n  sns.set(font_scale=1.4) \n  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n  ax.set_xlabel('Predicted Action')\n  ax.set_ylabel('Actual Action')\n  plt.xticks(rotation=90)\n  plt.yticks(rotation=0)\n  ax.xaxis.set_ticklabels(labels)\n  ax.yaxis.set_ticklabels(labels)\n\n  plt.savefig(filename, bbox_inches='tight')\n  print(f\"Confusion matrix saved to {filename}\")\n  plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:39:40.137555Z","iopub.execute_input":"2025-06-05T15:39:40.138083Z","iopub.status.idle":"2025-06-05T15:39:40.143944Z","shell.execute_reply.started":"2025-06-05T15:39:40.138059Z","shell.execute_reply":"2025-06-05T15:39:40.143260Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"label_names = list(train_generator.class_names)\nactual, predicted = get_actual_predicted_labels(test_ds)\nplot_confusion_matrix(actual, predicted, label_names, 'test')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:39:40.144728Z","iopub.execute_input":"2025-06-05T15:39:40.144970Z","iopub.status.idle":"2025-06-05T15:40:13.480225Z","shell.execute_reply.started":"2025-06-05T15:39:40.144949Z","shell.execute_reply":"2025-06-05T15:40:13.479401Z"}},"outputs":[{"name":"stdout","text":"17/17 [==============================] - 32s 800ms/step\nConfusion matrix saved to confusion_matrix.png\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n\nprint(classification_report(actual, predicted, target_names=label_names))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:40:13.481022Z","iopub.execute_input":"2025-06-05T15:40:13.481284Z","iopub.status.idle":"2025-06-05T15:40:13.493921Z","shell.execute_reply.started":"2025-06-05T15:40:13.481257Z","shell.execute_reply":"2025-06-05T15:40:13.493210Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n    action_0       0.79      1.00      0.88        15\n    action_1       1.00      0.67      0.80        15\n    action_2       1.00      0.87      0.93        15\n    action_3       0.94      1.00      0.97        15\n    action_4       0.88      1.00      0.94        15\n    action_5       0.88      1.00      0.94        15\n    action_6       1.00      0.93      0.97        15\n    action_7       1.00      1.00      1.00        12\n    action_8       1.00      0.93      0.97        15\n\n    accuracy                           0.93       132\n   macro avg       0.94      0.93      0.93       132\nweighted avg       0.94      0.93      0.93       132\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"\n# --- Streaming Inference Example ---\nprint(\"\\nSetting up model for streaming inference...\")\n# 1. Create backbone for streaming (causal=True, use_external_states=True)\nstreaming_backbone = movinet.Movinet(\n    model_id=model_id,\n    causal=True,\n    use_external_states=True # Crucial for streaming\n)\n# 2. Create classifier for streaming (output_states=True)\nstreaming_model = movinet_model.MovinetClassifier(\n    backbone=streaming_backbone,\n    num_classes=num_classes,\n    output_states=True # Crucial for getting states back\n)\n\n# 3. Build the streaming model by calling it once\n# This ensures layers are built and input/state shapes are understood.\nstreaming_batch_size_build = 1\ndummy_frame_shape_for_build = [streaming_batch_size_build, 1, resolution, resolution, 3] # B, T=1, H, W, C\ndummy_input_frame = tf.zeros(dummy_frame_shape_for_build, dtype=tf.float32)\ntry:\n    # Initialize states for the specific input shape for the streaming model\n    initial_build_states = streaming_model.init_states(tf.TensorShape(dummy_frame_shape_for_build))\n    print(\"Building streaming model by performing a dummy call...\")\n    _ = streaming_model((dummy_input_frame, initial_build_states), training=False) # Call to build\n    print(\"Streaming model built successfully.\")\nexcept Exception as e:\n    print(f\"Error building streaming model with a dummy call: {e}\")\n    print(\"Attempting to build with model.build() as fallback (might be less robust for tuple inputs).\")\n    # Fallback build (less ideal for models expecting tuple inputs like (frames, states))\n    # For MovinetClassifier, the primary input is frames, states are handled internally if use_external_states=False\n    # but for use_external_states=True, the call signature expects (frames, states).\n    # The dummy call above is generally better. If it fails, there might be deeper issues.\n    # This build command might not correctly infer the tuple input structure.\n    # streaming_model.build(input_shape=[dummy_frame_shape_for_build, type(initial_build_states)]) # This is complex\n    # For now, if dummy call fails, we might not be able to proceed with set_weights reliably.\n\n# 4. Transfer weights from the fine-tuned training model to the streaming model\nprint(\"Transferring all weights from fine-tuned training model to streaming model...\")\ntry:\n    streaming_model.set_weights(model_for_training.get_weights())\n    print(\"Successfully transferred weights to streaming model.\")\nexcept Exception as e:\n    print(f\"Error transferring weights to streaming model: {e}\")\n    print(\"Ensure streaming model was built correctly before setting weights.\")\n\n\nif not test_df.empty:\n    example_video_path_str = test_df['video_path'].iloc[0]\n    example_video_path = data_dir / example_video_path_str\n    \n    print(f\"\\nRunning streaming inference demo on: {example_video_path}\")\n\n    streaming_batch_size_demo = 1 \n    single_frame_input_shape = tf.TensorShape([streaming_batch_size_demo, 1, resolution, resolution, 3])\n    \n    try:\n        states = streaming_model.init_states(single_frame_input_shape)\n    except Exception as e:\n        print(f\"Error initializing states for streaming demo model: {e}\")\n        states = None # Cannot proceed with demo if states cannot be initialized\n\n    if states is not None:\n        cap = cv2.VideoCapture(str(example_video_path))\n        if not cap.isOpened():\n            print(f\"Error: Could not open video file {example_video_path} for streaming demo.\")\n        else:\n            num_demo_frames = n_frames * 2 \n            for i in range(num_demo_frames):\n                ret, frame_bgr = cap.read()\n                if not ret:\n                    print(\"End of video or error reading frame during streaming demo.\")\n                    break\n                \n                frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n                formatted_frame_tensor = format_frames(frame_rgb, (resolution, resolution))\n                input_frame_stream = formatted_frame_tensor[tf.newaxis, tf.newaxis, ...] \n\n                try:\n                    logits, states = streaming_model((input_frame_stream, states), training=False)\n                    predicted_class_id = tf.argmax(logits, axis=-1).numpy()[0]\n                    predicted_label_name = class_names_list[predicted_class_id] if predicted_class_id < len(class_names_list) else f\"unknown_id_{predicted_class_id}\"\n                    print(f\"Stream Frame {i+1}: Predicted Class ID: {predicted_class_id}, Label: {predicted_label_name}\")\n                except Exception as e:\n                    print(f\"Error during streaming inference on frame {i+1}: {e}\")\n                    break \n            cap.release()\nelse:\n    print(\"Test set is empty. Skipping streaming inference demo.\")\n\n# To save the fine-tuned streaming model (which is `streaming_model`):\n# print(\"Saving the fine-tuned streaming model...\")\n# streaming_model.save('my_streaming_movinet_finetuned_model') # Saves in SavedModel format\n# Note: When loading a SavedModel with custom objects (like MoViNet layers),\n# you might need to provide them via custom_objects argument or ensure they are registered.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:40:13.494641Z","iopub.execute_input":"2025-06-05T15:40:13.494890Z","iopub.status.idle":"2025-06-05T15:47:01.217001Z","shell.execute_reply.started":"2025-06-05T15:40:13.494868Z","shell.execute_reply":"2025-06-05T15:47:01.216276Z"}},"outputs":[{"name":"stdout","text":"\nSetting up model for streaming inference...\nBuilding streaming model by performing a dummy call...\nStreaming model built successfully.\nTransferring all weights from fine-tuned training model to streaming model...\nSuccessfully transferred weights to streaming model.\n\nRunning streaming inference demo on: /kaggle/input/changing-tires/remove_tire/remove_tire_04_piecewise_affine.mp4\nStream Frame 1: Predicted Class ID: 4, Label: action_4\nStream Frame 2: Predicted Class ID: 5, Label: action_5\nStream Frame 3: Predicted Class ID: 7, Label: action_7\nStream Frame 4: Predicted Class ID: 5, Label: action_5\nStream Frame 5: Predicted Class ID: 7, Label: action_7\nStream Frame 6: Predicted Class ID: 7, Label: action_7\nStream Frame 7: Predicted Class ID: 7, Label: action_7\nStream Frame 8: Predicted Class ID: 7, Label: action_7\nStream Frame 9: Predicted Class ID: 6, Label: action_6\nStream Frame 10: Predicted Class ID: 6, Label: action_6\nStream Frame 11: Predicted Class ID: 6, Label: action_6\nStream Frame 12: Predicted Class ID: 6, Label: action_6\nStream Frame 13: Predicted Class ID: 6, Label: action_6\nStream Frame 14: Predicted Class ID: 6, Label: action_6\nStream Frame 15: Predicted Class ID: 6, Label: action_6\nStream Frame 16: Predicted Class ID: 6, Label: action_6\nStream Frame 17: Predicted Class ID: 6, Label: action_6\nStream Frame 18: Predicted Class ID: 6, Label: action_6\nStream Frame 19: Predicted Class ID: 6, Label: action_6\nStream Frame 20: Predicted Class ID: 6, Label: action_6\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"list(action_to_label.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:47:01.217848Z","iopub.execute_input":"2025-06-05T15:47:01.218124Z","iopub.status.idle":"2025-06-05T15:47:01.224028Z","shell.execute_reply.started":"2025-06-05T15:47:01.218104Z","shell.execute_reply":"2025-06-05T15:47:01.223312Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"['hand_tighten_bolts',\n 'initial_wrench_tighten',\n 'lift_car_with_jack',\n 'loosen_bolts',\n 'lower_car',\n 'place_spare_tire',\n 'remove_bolts',\n 'remove_tire',\n 'tighten_bolts']"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# ========== SAVING THE STREAMING MODEL (Weights Method) ==========\nprint(\"Saving the fine-tuned streaming model weights...\")\n\n# Save model weights\nstreaming_model.save_weights('streaming_movinet_weights.h5')\nprint(\"✅ Streaming model weights saved!\")\n\n# Create and save model configuration \n# Replace these with your actual values if different\nimport json\nconfig = {\n    'model_id': 'a0',  \n    'num_classes': 9,  # Change this to your number of classes\n    'resolution': 224,  # Change this to your resolution (e.g., 224, 172, etc.)\n    'class_names': list(action_to_label.keys()),  # Replace with your actual class names\n    'streaming_batch_size': 1\n}\n\n# Print current values so you can verify them\nprint(f\"📋 Saving configuration:\")\nprint(f\"   Model ID: {config['model_id']}\")\nprint(f\"   Num classes: {config['num_classes']}\")\nprint(f\"   Resolution: {config['resolution']}\")\nprint(f\"   Class names: {config['class_names']}\")\n\nwith open('/kaggle/working/streaming_model_config.json', 'w') as f:\n    json.dump(config, f, indent=2)\nprint(\"✅ Model configuration saved!\")\n\n# ========== LOADING AND RECONSTRUCTING THE MODEL ==========\nimport tensorflow as tf\nimport cv2\nimport numpy as np\nimport json\nfrom official.projects.movinet.modeling import movinet\nfrom official.projects.movinet.modeling import movinet_model\n\n# Load configuration\nwith open('/kaggle/working/streaming_model_config.json', 'r') as f:\n    config = json.load(f)\n\nmodel_id = config['model_id']\nnum_classes = config['num_classes']\nresolution = config['resolution']\nclass_names_list = config['class_names']\nstreaming_batch_size = config['streaming_batch_size']\n\nprint(\"Reconstructing streaming model architecture...\")\n\n# 1. Create streaming backbone (same as original)\nstreaming_backbone = movinet.Movinet(\n    model_id=model_id,\n    causal=True,\n    use_external_states=True\n)\n\n# 2. Create streaming classifier (same as original)\nloaded_streaming_model = movinet_model.MovinetClassifier(\n    backbone=streaming_backbone,\n    num_classes=num_classes,\n    output_states=True\n)\n\n# 3. Build the model by calling it once (same as original)\ndummy_frame_shape = [streaming_batch_size, 1, resolution, resolution, 3]\ndummy_input_frame = tf.zeros(dummy_frame_shape, dtype=tf.float32)\n\ntry:\n    initial_states = loaded_streaming_model.init_states(tf.TensorShape(dummy_frame_shape))\n    print(\"Building streaming model by performing a dummy call...\")\n    _ = loaded_streaming_model((dummy_input_frame, initial_states), training=False)\n    print(\"Streaming model built successfully.\")\nexcept Exception as e:\n    print(f\"Error building streaming model: {e}\")\n    raise e\n\n# 4. Load the saved weights\nprint(\"Loading saved weights...\")\nloaded_streaming_model.load_weights('/kaggle/working/streaming_movinet_weights.h5')\nprint(\"✅ Weights loaded successfully!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:47:01.224817Z","iopub.execute_input":"2025-06-05T15:47:01.225013Z","iopub.status.idle":"2025-06-05T15:47:32.447346Z","shell.execute_reply.started":"2025-06-05T15:47:01.224999Z","shell.execute_reply":"2025-06-05T15:47:32.446773Z"}},"outputs":[{"name":"stdout","text":"Saving the fine-tuned streaming model weights...\n✅ Streaming model weights saved!\n📋 Saving configuration:\n   Model ID: a0\n   Num classes: 9\n   Resolution: 224\n   Class names: ['hand_tighten_bolts', 'initial_wrench_tighten', 'lift_car_with_jack', 'loosen_bolts', 'lower_car', 'place_spare_tire', 'remove_bolts', 'remove_tire', 'tighten_bolts']\n✅ Model configuration saved!\nReconstructing streaming model architecture...\nBuilding streaming model by performing a dummy call...\nStreaming model built successfully.\nLoading saved weights...\n✅ Weights loaded successfully!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(loaded_streaming_model.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:47:32.448155Z","iopub.execute_input":"2025-06-05T15:47:32.448775Z","iopub.status.idle":"2025-06-05T15:47:32.668431Z","shell.execute_reply.started":"2025-06-05T15:47:32.448747Z","shell.execute_reply":"2025-06-05T15:47:32.667923Z"}},"outputs":[{"name":"stdout","text":"Model: \"movinet_classifier_3\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n image (InputLayer)          [(None, None, None, None,    0         []                            \n                             3)]                                                                  \n                                                                                                  \n state_block0_layer0_pool_b  [(None, 1, 1, 1, 24)]        0         []                            \n uffer (InputLayer)                                                                               \n                                                                                                  \n state_block0_layer0_pool_f  [(None,)]                    0         []                            \n rame_count (InputLayer)                                                                          \n                                                                                                  \n state_block1_layer0_pool_b  [(None, 1, 1, 1, 80)]        0         []                            \n uffer (InputLayer)                                                                               \n                                                                                                  \n state_block1_layer0_pool_f  [(None,)]                    0         []                            \n rame_count (InputLayer)                                                                          \n                                                                                                  \n state_block1_layer0_stream  [(None, 2, None, None, 80)   0         []                            \n _buffer (InputLayer)        ]                                                                    \n                                                                                                  \n state_block1_layer1_pool_b  [(None, 1, 1, 1, 80)]        0         []                            \n uffer (InputLayer)                                                                               \n                                                                                                  \n state_block1_layer1_pool_f  [(None,)]                    0         []                            \n rame_count (InputLayer)                                                                          \n                                                                                                  \n state_block1_layer1_stream  [(None, 2, None, None, 80)   0         []                            \n _buffer (InputLayer)        ]                                                                    \n                                                                                                  \n state_block1_layer2_pool_b  [(None, 1, 1, 1, 80)]        0         []                            \n uffer (InputLayer)                                                                               \n                                                                                                  \n state_block1_layer2_pool_f  [(None,)]                    0         []                            \n rame_count (InputLayer)                                                                          \n                                                                                                  \n state_block1_layer2_stream  [(None, 2, None, None, 80)   0         []                            \n _buffer (InputLayer)        ]                                                                    \n                                                                                                  \n state_block2_layer0_pool_b  [(None, 1, 1, 1, 184)]       0         []                            \n uffer (InputLayer)                                                                               \n                                                                                                  \n state_block2_layer0_pool_f  [(None,)]                    0         []                            \n rame_count (InputLayer)                                                                          \n                                                                                                  \n state_block2_layer0_stream  [(None, 4, None, None, 184   0         []                            \n _buffer (InputLayer)        )]                                                                   \n                                                                                                  \n state_block2_layer1_pool_b  [(None, 1, 1, 1, 112)]       0         []                            \n uffer (InputLayer)                                                                               \n                                                                                                  \n state_block2_layer1_pool_f  [(None,)]                    0         []                            \n rame_count (InputLayer)                                                                          \n                                                                                                  \n state_block2_layer1_stream  [(None, 2, None, None, 112   0         []                            \n _buffer (InputLayer)        )]                                                                   \n                                                                                                  \n state_block2_layer2_pool_b  [(None, 1, 1, 1, 184)]       0         []                            \n uffer (InputLayer)                                                                               \n                                                                                                  \n state_block2_layer2_pool_f  [(None,)]                    0         []                            \n rame_count (InputLayer)                                                                          \n                                                                                                  \n state_block2_layer2_stream  [(None, 2, None, None, 184   0         []                            \n _buffer (InputLayer)        )]                                                                   \n                                                                                                  \n state_block3_layer0_pool_b  [(None, 1, 1, 1, 184)]       0         []                            \n uffer (InputLayer)                                                                               \n                                                                                                  \n state_block3_layer0_pool_f  [(None,)]                    0         []                            \n rame_count (InputLayer)                                                                          \n                                                                                                  \n state_block3_layer0_stream  [(None, 4, None, None, 184   0         []                            \n _buffer (InputLayer)        )]                                                                   \n                                                                                                  \n state_block3_layer1_pool_b  [(None, 1, 1, 1, 184)]       0         []                            \n uffer (InputLayer)                                                                               \n                                                                                                  \n state_block3_layer1_pool_f  [(None,)]                    0         []                            \n rame_count (InputLayer)                                                                          \n                                                                                                  \n state_block3_layer1_stream  [(None, 2, None, None, 184   0         []                            \n _buffer (InputLayer)        )]                                                                   \n                                                                                                  \n state_block3_layer2_pool_b  [(None, 1, 1, 1, 184)]       0         []                            \n uffer (InputLayer)                                                                               \n                                                                                                  \n state_block3_layer2_pool_f  [(None,)]                    0         []                            \n rame_count (InputLayer)                                                                          \n                                                                                                  \n state_block3_layer2_stream  [(None, 2, None, None, 184   0         []                            \n _buffer (InputLayer)        )]                                                                   \n                                                                                                  \n state_block3_layer3_pool_b  [(None, 1, 1, 1, 184)]       0         []                            \n uffer (InputLayer)                                                                               \n                                                                                                  \n state_block3_layer3_pool_f  [(None,)]                    0         []                            \n rame_count (InputLayer)                                                                          \n                                                                                                  \n state_block3_layer3_stream  [(None, 2, None, None, 184   0         []                            \n _buffer (InputLayer)        )]                                                                   \n                                                                                                  \n state_block4_layer0_pool_b  [(None, 1, 1, 1, 384)]       0         []                            \n uffer (InputLayer)                                                                               \n                                                                                                  \n state_block4_layer0_pool_f  [(None,)]                    0         []                            \n rame_count (InputLayer)                                                                          \n                                                                                                  \n state_block4_layer0_stream  [(None, 4, None, None, 384   0         []                            \n _buffer (InputLayer)        )]                                                                   \n                                                                                                  \n state_block4_layer1_pool_b  [(None, 1, 1, 1, 280)]       0         []                            \n uffer (InputLayer)                                                                               \n                                                                                                  \n state_block4_layer1_pool_f  [(None,)]                    0         []                            \n rame_count (InputLayer)                                                                          \n                                                                                                  \n state_block4_layer2_pool_b  [(None, 1, 1, 1, 280)]       0         []                            \n uffer (InputLayer)                                                                               \n                                                                                                  \n state_block4_layer2_pool_f  [(None,)]                    0         []                            \n rame_count (InputLayer)                                                                          \n                                                                                                  \n state_block4_layer3_pool_b  [(None, 1, 1, 1, 344)]       0         []                            \n uffer (InputLayer)                                                                               \n                                                                                                  \n state_block4_layer3_pool_f  [(None,)]                    0         []                            \n rame_count (InputLayer)                                                                          \n                                                                                                  \n state_head_pool_buffer (In  [(None, 1, 1, 1, 480)]       0         []                            \n putLayer)                                                                                        \n                                                                                                  \n state_head_pool_frame_coun  [(None,)]                    0         []                            \n t (InputLayer)                                                                                   \n                                                                                                  \n movinet_3 (Movinet)         ({'stem': (None, None, Non   911583    ['image[0][0]',               \n                             e, None, 8),                            'state_block0_layer0_pool_buf\n                              'block0_layer0': (None, N             fer[0][0]',                   \n                             one, None, None, 8),                    'state_block0_layer0_pool_fra\n                              'block1_layer0': (None, N             me_count[0][0]',              \n                              'block0_layer0': (None, N             fer[0][0]',                   \n                             one, None, None, 8),                    'state_block0_layer0_pool_fra\n                              'block1_layer0': (None, N             me_count[0][0]',              \n                             one, None, None, 32),                   'state_block1_layer0_pool_buf\n                              'block1_layer1': (None, N             fer[0][0]',                   \n                             one, None, None, 32),                   'state_block1_layer0_pool_fra\n                              'block1_layer2': (None, N             me_count[0][0]',              \n                             one, None, None, 32),                   'state_block1_layer0_stream_b\n                              'block2_layer0': (None, N             uffer[0][0]',                 \n                             one, None, None, 56),                   'state_block1_layer1_pool_buf\n                              'block2_layer1': (None, N             fer[0][0]',                   \n                             one, None, None, 56),                   'state_block1_layer1_pool_fra\n                              'block2_layer2': (None, N             me_count[0][0]',              \n                             one, None, None, 56),                   'state_block1_layer1_stream_b\n                              'block3_layer0': (None, N             uffer[0][0]',                 \n                             one, None, None, 56),                   'state_block1_layer2_pool_buf\n                              'block3_layer1': (None, N             fer[0][0]',                   \n                             one, None, None, 56),                   'state_block1_layer2_pool_fra\n                              'block3_layer2': (None, N             me_count[0][0]',              \n                             one, None, None, 56),                   'state_block1_layer2_stream_b\n                              'block3_layer3': (None, N             uffer[0][0]',                 \n                             one, None, None, 56),                   'state_block2_layer0_pool_buf\n                              'block4_layer0': (None, N             fer[0][0]',                   \n                             one, None, None, 104),                  'state_block2_layer0_pool_fra\n                              'block4_layer1': (None, N             me_count[0][0]',              \n                             one, None, None, 104),                  'state_block2_layer0_stream_b\n                              'block4_layer2': (None, N             uffer[0][0]',                 \n                             one, None, None, 104),                  'state_block2_layer1_pool_buf\n                              'block4_layer3': (None, N             fer[0][0]',                   \n                             one, None, None, 104),                  'state_block2_layer1_pool_fra\n                              'head': (None, 1, 1, 1, 4             me_count[0][0]',              \n                             80)},                                   'state_block2_layer1_stream_b\n                              {'state_block0_layer0_poo             uffer[0][0]',                 \n                             l_buffer': (None, None, 1,              'state_block2_layer2_pool_buf\n                              1, 24),                               fer[0][0]',                   \n                              'state_block0_layer0_pool              'state_block2_layer2_pool_fra\n                             _frame_count': (None,),                me_count[0][0]',              \n                              'state_block1_layer0_stre              'state_block2_layer2_stream_b\n                             am_buffer': (None, None, N             uffer[0][0]',                 \n                             one, None, 80),                         'state_block3_layer0_pool_buf\n                              'state_block1_layer0_pool             fer[0][0]',                   \n                             _buffer': (None, None, 1,               'state_block3_layer0_pool_fra\n                             1, 80),                                me_count[0][0]',              \n                              'state_block1_layer0_pool              'state_block3_layer0_stream_b\n                             _frame_count': (None,),                uffer[0][0]',                 \n                              'state_block1_layer1_stre              'state_block3_layer1_pool_buf\n                             am_buffer': (None, None, N             fer[0][0]',                   \n                             one, None, 80),                         'state_block3_layer1_pool_fra\n                              'state_block1_layer1_pool             me_count[0][0]',              \n                             _buffer': (None, None, 1,               'state_block3_layer1_stream_b\n                             1, 80),                                uffer[0][0]',                 \n                              'state_block1_layer1_pool              'state_block3_layer2_pool_buf\n                             _frame_count': (None,),                fer[0][0]',                   \n                              'state_block1_layer2_stre              'state_block3_layer2_pool_fra\n                             am_buffer': (None, None, N             me_count[0][0]',              \n                             one, None, 80),                         'state_block3_layer2_stream_b\n                              'state_block1_layer2_pool             uffer[0][0]',                 \n                             _buffer': (None, None, 1,               'state_block3_layer3_pool_buf\n                             1, 80),                                fer[0][0]',                   \n                              'state_block1_layer2_pool              'state_block3_layer3_pool_fra\n                             _frame_count': (None,),                me_count[0][0]',              \n                              'state_block2_layer0_stre              'state_block3_layer3_stream_b\n                             am_buffer': (None, None, N             uffer[0][0]',                 \n                             one, None, 184),                        'state_block4_layer0_pool_buf\n                              'state_block2_layer0_pool             fer[0][0]',                   \n                             _buffer': (None, None, 1,               'state_block4_layer0_pool_fra\n                             1, 184),                               me_count[0][0]',              \n                              'state_block2_layer0_pool              'state_block4_layer0_stream_b\n                             _frame_count': (None,),                uffer[0][0]',                 \n                              'state_block2_layer1_stre              'state_block4_layer1_pool_buf\n                             am_buffer': (None, None, N             fer[0][0]',                   \n                             one, None, 112),                        'state_block4_layer1_pool_fra\n                              'state_block2_layer1_pool             me_count[0][0]',              \n                             _buffer': (None, None, 1,               'state_block4_layer2_pool_buf\n                             1, 112),                               fer[0][0]',                   \n                              'state_block2_layer1_pool              'state_block4_layer2_pool_fra\n                             _frame_count': (None,),                me_count[0][0]',              \n                             one, None, 184),                        'state_block4_layer3_pool_fra\n                              'state_block2_layer2_pool             me_count[0][0]',              \n                             _buffer': (None, None, 1,               'state_head_pool_buffer[0][0]\n                             1, 184),                               ',                            \n                              'state_block2_layer2_pool              'state_head_pool_frame_count[\n                             _frame_count': (None,),                0][0]']                       \n                              'state_block3_layer0_stre                                           \n                             am_buffer': (None, None, N                                           \n                             one, None, 184),                                                     \n                              'state_block3_layer0_pool                                           \n                             _buffer': (None, None, 1,                                            \n                             1, 184),                                                             \n                              'state_block3_layer0_pool                                           \n                             _frame_count': (None,),                                              \n                              'state_block3_layer1_stre                                           \n                             am_buffer': (None, None, N                                           \n                             one, None, 184),                                                     \n                              'state_block3_layer1_pool                                           \n                             _buffer': (None, None, 1,                                            \n                             1, 184),                                                             \n                              'state_block3_layer1_pool                                           \n                             _frame_count': (None,),                                              \n                              'state_block3_layer2_stre                                           \n                             am_buffer': (None, None, N                                           \n                             one, None, 184),                                                     \n                              'state_block3_layer2_pool                                           \n                             _buffer': (None, None, 1,                                            \n                             1, 184),                                                             \n                              'state_block3_layer2_pool                                           \n                             _frame_count': (None,),                                              \n                              'state_block3_layer3_stre                                           \n                             am_buffer': (None, None, N                                           \n                             one, None, 184),                                                     \n                              'state_block3_layer3_pool                                           \n                             _buffer': (None, None, 1,                                            \n                             1, 184),                                                             \n                              'state_block3_layer3_pool                                           \n                             _frame_count': (None,),                                              \n                              'state_block4_layer0_stre                                           \n                             am_buffer': (None, None, N                                           \n                             one, None, 384),                                                     \n                              'state_block4_layer0_pool                                           \n                             _buffer': (None, None, 1,                                            \n                             1, 384),                                                             \n                              'state_block4_layer0_pool                                           \n                             _frame_count': (None,),                                              \n                              'state_block4_layer1_pool                                           \n                             _buffer': (None, None, 1,                                            \n                             1, 280),                                                             \n                              'state_block4_layer1_pool                                           \n                             _frame_count': (None,),                                              \n                              'state_block4_layer2_pool                                           \n                             _buffer': (None, None, 1,                                            \n                             1, 280),                                                             \n                              'state_block4_layer2_pool                                           \n                             _frame_count': (None,),                                              \n                              'state_block4_layer3_pool                                           \n                             _buffer': (None, None, 1,                                            \n                             1, 344),                                                             \n                              'state_block4_layer3_pool                                           \n                             _frame_count': (None,),                                              \n                              'state_head_pool_buffer':                                           \n                              (None, 1, 1, 1, 480),                                               \n                              'state_head_pool_frame_co                                           \n                             unt': (None,)})                                                      \n                                                                                                  \n classifier_head_3 (Classif  (None, 9)                    1003529   ['movinet_3[0][15]']          \n ierHead)                                                                                         \n                                                                                                  \n==================================================================================================\nTotal params: 1915112 (7.31 MB)\nTrainable params: 1900840 (7.25 MB)\nNon-trainable params: 14272 (55.75 KB)\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ========== FORMAT FRAMES FUNCTION ==========\ndef format_frames(frame, output_size):\n    \"\"\"Format frame for model input (same as your original)\"\"\"\n    frame = tf.image.convert_image_dtype(frame, tf.float32)\n    frame = tf.image.resize_with_pad(frame, *output_size)\n    return frame\n\n# ========== VIDEO PREDICTION FUNCTION ==========\ndef predict_video_streaming(model, video_path, class_names, resolution=224, max_frames=None):\n    \"\"\"Streaming video prediction (inspired by your original demo)\"\"\"\n    print(f\"\\nRunning streaming inference on: {video_path}\")\n    \n    # Initialize states for streaming\n    single_frame_input_shape = tf.TensorShape([1, 1, resolution, resolution, 3])\n    \n    try:\n        states = model.init_states(single_frame_input_shape)\n        print(\"States initialized successfully.\")\n    except Exception as e:\n        print(f\"Error initializing states: {e}\")\n        return []\n    \n    # Open video\n    cap = cv2.VideoCapture(str(video_path))\n    if not cap.isOpened():\n        print(f\"Error: Could not open video file {video_path}\")\n        return []\n    \n    predictions = []\n    frame_count = 0\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    process_frames = min(total_frames, max_frames) if max_frames else total_frames\n    \n    print(f\"Processing {process_frames} frames...\")\n    \n    while frame_count < process_frames:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            print(\"End of video or error reading frame.\")\n            break\n        \n        # Convert BGR to RGB (same as original)\n        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n        \n        # Format frame (same as original)\n        formatted_frame_tensor = format_frames(frame_rgb, (resolution, resolution))\n        input_frame_stream = formatted_frame_tensor[tf.newaxis, tf.newaxis, ...]\n        \n        try:\n            # Streaming inference (same as original)\n            logits, states = model((input_frame_stream, states), training=False)\n            predicted_class_id = tf.argmax(logits, axis=-1).numpy()[0]\n            predicted_label_name = class_names[predicted_class_id] if predicted_class_id < len(class_names) else f\"unknown_id_{predicted_class_id}\"\n            \n            # Get confidence\n            confidence = tf.nn.softmax(logits).numpy()[0][predicted_class_id]\n            \n            prediction_info = {\n                'frame': frame_count + 1,\n                'class_id': int(predicted_class_id),\n                'class_name': predicted_label_name,\n                'confidence': float(confidence)\n            }\n            \n            predictions.append(prediction_info)\n            \n            # Print progress (same format as original)\n            print(f\"Stream Frame {frame_count + 1}: Predicted Class ID: {predicted_class_id}, Label: {predicted_label_name}, Confidence: {confidence:.3f}\")\n            \n        except Exception as e:\n            print(f\"Error during streaming inference on frame {frame_count + 1}: {e}\")\n            break\n        \n        frame_count += 1\n    \n    cap.release()\n    print(f\"\\nStreaming inference completed! Processed {len(predictions)} frames\")\n    return predictions\n\n# ========== USAGE EXAMPLE ==========\n# Replace with your actual video path\nvideo_path = \"/kaggle/input/changing-tires/hand_tighten_bolts/hand_tighten_bolts_13.mp4\"\n\n# Check if video exists\nfrom pathlib import Path\nif Path(video_path).exists():\n    # Run streaming prediction\n    results = predict_video_streaming(\n        model=loaded_streaming_model,\n        video_path=video_path,\n        class_names=class_names_list,\n        resolution=resolution,\n        max_frames=20  # Process first 50 frames, set to None for all frames\n    )\n    \n    # Show summary\n    if results:\n        print(f\"\\n📊 PREDICTION SUMMARY:\")\n        print(f\"Total frames: {len(results)}\")\n        \n        # Most common prediction\n        from collections import Counter\n        class_counts = Counter([r['class_name'] for r in results])\n        most_common = class_counts.most_common(1)[0]\n        print(f\"Most predicted class: {most_common[0]} ({most_common[1]} frames)\")\n        \n        # Average confidence\n        avg_confidence = sum([r['confidence'] for r in results]) / len(results)\n        print(f\"Average confidence: {avg_confidence:.3f}\")\n        \n        \nelse:\n    print(f\"❌ Video file not found: {video_path}\")\n    print(\"Please update the video_path variable with your actual video file path\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:47:32.669173Z","iopub.execute_input":"2025-06-05T15:47:32.669348Z","iopub.status.idle":"2025-06-05T15:53:47.410706Z","shell.execute_reply.started":"2025-06-05T15:47:32.669335Z","shell.execute_reply":"2025-06-05T15:53:47.409951Z"}},"outputs":[{"name":"stdout","text":"\nRunning streaming inference on: /kaggle/input/changing-tires/hand_tighten_bolts/hand_tighten_bolts_13.mp4\nStates initialized successfully.\nProcessing 20 frames...\nStream Frame 1: Predicted Class ID: 4, Label: lower_car, Confidence: 0.182\nStream Frame 2: Predicted Class ID: 4, Label: lower_car, Confidence: 0.196\nStream Frame 3: Predicted Class ID: 5, Label: place_spare_tire, Confidence: 0.325\nStream Frame 4: Predicted Class ID: 5, Label: place_spare_tire, Confidence: 0.563\nStream Frame 5: Predicted Class ID: 5, Label: place_spare_tire, Confidence: 0.653\nStream Frame 6: Predicted Class ID: 5, Label: place_spare_tire, Confidence: 0.510\nStream Frame 7: Predicted Class ID: 0, Label: hand_tighten_bolts, Confidence: 0.568\nStream Frame 8: Predicted Class ID: 0, Label: hand_tighten_bolts, Confidence: 0.623\nStream Frame 9: Predicted Class ID: 0, Label: hand_tighten_bolts, Confidence: 0.683\nStream Frame 10: Predicted Class ID: 0, Label: hand_tighten_bolts, Confidence: 0.733\nStream Frame 11: Predicted Class ID: 0, Label: hand_tighten_bolts, Confidence: 0.746\nStream Frame 12: Predicted Class ID: 0, Label: hand_tighten_bolts, Confidence: 0.767\nStream Frame 13: Predicted Class ID: 0, Label: hand_tighten_bolts, Confidence: 0.797\nStream Frame 14: Predicted Class ID: 0, Label: hand_tighten_bolts, Confidence: 0.832\nStream Frame 15: Predicted Class ID: 0, Label: hand_tighten_bolts, Confidence: 0.854\nStream Frame 16: Predicted Class ID: 0, Label: hand_tighten_bolts, Confidence: 0.859\nStream Frame 17: Predicted Class ID: 0, Label: hand_tighten_bolts, Confidence: 0.852\nStream Frame 18: Predicted Class ID: 0, Label: hand_tighten_bolts, Confidence: 0.842\nStream Frame 19: Predicted Class ID: 0, Label: hand_tighten_bolts, Confidence: 0.832\nStream Frame 20: Predicted Class ID: 0, Label: hand_tighten_bolts, Confidence: 0.824\n\nStreaming inference completed! Processed 20 frames\n\n📊 PREDICTION SUMMARY:\nTotal frames: 20\nMost predicted class: hand_tighten_bolts (14 frames)\nAverage confidence: 0.662\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.animation import FuncAnimation, PillowWriter\nimport tensorflow as tf\nfrom pathlib import Path\nfrom collections import Counter, deque\nimport json\n\ndef format_frames(frame, output_size):\n    \"\"\"Format frame for model input\"\"\"\n    frame = tf.image.convert_image_dtype(frame, tf.float32)\n    frame = tf.image.resize_with_pad(frame, *output_size)\n    return frame\n\ndef create_prediction_video_with_plot(model, video_path, class_names, resolution=224, \n                                    max_frames=None, output_path='/kaggle/working/prediction_video.mp4'):\n    \"\"\"\n    Create a video showing original frames + real-time prediction plot\n    \"\"\"\n    print(f\"\\nCreating prediction visualization for: {video_path}\")\n    \n    # Initialize states for streaming\n    single_frame_input_shape = tf.TensorShape([1, 1, resolution, resolution, 3])\n    \n    try:\n        states = model.init_states(single_frame_input_shape)\n        print(\"States initialized successfully.\")\n    except Exception as e:\n        print(f\"Error initializing states: {e}\")\n        return None\n    \n    # Open input video\n    cap = cv2.VideoCapture(str(video_path))\n    if not cap.isOpened():\n        print(f\"Error: Could not open video file {video_path}\")\n        return None\n    \n    # Get video properties\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    process_frames = min(total_frames, max_frames) if max_frames else total_frames\n    print(f\"Processing {process_frames} frames at {fps} FPS...\")\n    \n    # Prepare data storage\n    predictions = []\n    confidence_history = []\n    class_history = []\n    frames_data = []\n    \n    # Colors for each class\n    colors = plt.cm.Set3(np.linspace(0, 1, len(class_names)))\n    class_colors = {class_name: colors[i] for i, class_name in enumerate(class_names)}\n    \n    frame_count = 0\n    \n    # Process all frames first\n    print(\"Processing frames for predictions...\")\n    while frame_count < process_frames:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n        \n        # Store original frame\n        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n        frames_data.append(frame_rgb.copy())\n        \n        # Format frame for prediction\n        formatted_frame_tensor = format_frames(frame_rgb, (resolution, resolution))\n        input_frame_stream = formatted_frame_tensor[tf.newaxis, tf.newaxis, ...]\n        \n        try:\n            # Streaming inference\n            logits, states = model((input_frame_stream, states), training=False)\n            predicted_class_id = tf.argmax(logits, axis=-1).numpy()[0]\n            predicted_label_name = class_names[predicted_class_id]\n            \n            # Get all class confidences\n            all_confidences = tf.nn.softmax(logits).numpy()[0]\n            max_confidence = all_confidences[predicted_class_id]\n            \n            prediction_info = {\n                'frame': frame_count + 1,\n                'class_id': int(predicted_class_id),\n                'class_name': predicted_label_name,\n                'confidence': float(max_confidence),\n                'all_confidences': all_confidences.tolist()\n            }\n            \n            predictions.append(prediction_info)\n            confidence_history.append(max_confidence)\n            class_history.append(predicted_label_name)\n            \n            if (frame_count + 1) % 10 == 0:\n                print(f\"Processed frame {frame_count + 1}/{process_frames}\")\n            \n        except Exception as e:\n            print(f\"Error during inference on frame {frame_count + 1}: {e}\")\n            break\n        \n        frame_count += 1\n    \n    cap.release()\n    \n    if not predictions:\n        print(\"No predictions generated!\")\n        return None\n    \n    print(f\"Generating visualization video with {len(predictions)} frames...\")\n    \n    # Setup video writer\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (1800, 600))  # Width, Height matching figsize\n    \n    # Window for moving average\n    window_size = min(10, len(predictions))\n    \n    for i in range(len(predictions)):\n        # Create new figure for each frame - this fixes the figsize issue\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n        plt.tight_layout(pad=3.0)\n        \n        # Current prediction data\n        current_pred = predictions[i]\n        current_frame = frames_data[i]\n        \n        # 1. Show original frame with prediction overlay\n        ax1.imshow(current_frame)\n        ax1.set_title(f'Frame {i+1}/{len(predictions)}\\nPrediction: {current_pred[\"class_name\"]}\\nConfidence: {current_pred[\"confidence\"]:.3f}', \n                     fontsize=12, fontweight='bold')\n        ax1.axis('off')\n        \n        # Add prediction text overlay\n        bbox_props = dict(boxstyle=\"round,pad=0.3\", facecolor=class_colors[current_pred[\"class_name\"]], alpha=0.8)\n        ax1.text(0.02, 0.98, f'{current_pred[\"class_name\"]}\\n{current_pred[\"confidence\"]:.3f}', \n                transform=ax1.transAxes, fontsize=14, fontweight='bold',\n                verticalalignment='top', bbox=bbox_props, color='white')\n        \n        # 2. Confidence over time\n        frames_so_far = list(range(1, i+2))\n        confidences_so_far = confidence_history[:i+1]\n        \n        ax2.plot(frames_so_far, confidences_so_far, 'b-', linewidth=2, alpha=0.7)\n        ax2.scatter(frames_so_far[-1], confidences_so_far[-1], color='red', s=100, zorder=5)\n        \n        # Moving average\n        if len(confidences_so_far) >= window_size:\n            moving_avg = []\n            for j in range(window_size-1, len(confidences_so_far)):\n                avg = np.mean(confidences_so_far[j-window_size+1:j+1])\n                moving_avg.append(avg)\n            \n            avg_frames = frames_so_far[window_size-1:]\n            ax2.plot(avg_frames, moving_avg, 'r--', linewidth=2, alpha=0.8, label=f'Moving Avg ({window_size})')\n            ax2.legend()\n        \n        ax2.set_xlabel('Frame Number')\n        ax2.set_ylabel('Confidence')\n        ax2.set_title('Prediction Confidence Over Time')\n        ax2.grid(True, alpha=0.3)\n        ax2.set_ylim(0, 1)\n        ax2.set_xlim(1, len(predictions))\n        \n        # 3. Class distribution and current class probabilities\n        # Top: Class distribution over time\n        ax3_top = ax3\n        class_counts = Counter(class_history[:i+1])\n        classes = list(class_counts.keys())\n        counts = list(class_counts.values())\n        \n        bars = ax3_top.bar(classes, counts, color=[class_colors[cls] for cls in classes], alpha=0.7)\n        ax3_top.set_xlabel('Classes')\n        ax3_top.set_ylabel('Frame Count')\n        ax3_top.set_title('Class Distribution Over Time')\n        ax3_top.tick_params(axis='x', rotation=45)\n        \n        # Highlight current prediction\n        for j, cls in enumerate(classes):\n            if cls == current_pred[\"class_name\"]:\n                bars[j].set_alpha(1.0)\n                bars[j].set_edgecolor('black')\n                bars[j].set_linewidth(2)\n        \n        # Add count labels on bars\n        for bar, count in zip(bars, counts):\n            height = bar.get_height()\n            ax3_top.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n                        f'{count}', ha='center', va='bottom', fontweight='bold')\n        \n        plt.tight_layout()\n        \n        # Convert matplotlib figure to opencv image\n        fig.canvas.draw()\n        img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n        img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n        img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        \n        # Resize to match video writer dimensions if needed\n        if img_bgr.shape[:2] != (600, 1800):\n            img_bgr = cv2.resize(img_bgr, (1800, 600))\n        \n        out.write(img_bgr)\n        \n        # Close the figure to free memory\n        plt.close(fig)\n        \n        if (i + 1) % 20 == 0:\n            print(f\"Generated frame {i + 1}/{len(predictions)} of visualization\")\n    \n    out.release()\n    \n    print(f\"✅ Visualization video saved to: {output_path}\")\n    \n    # Save detailed results\n    results_data = {\n        'video_info': {\n            'original_path': str(video_path),\n            'total_frames_processed': len(predictions),\n            'fps': fps,\n            'resolution': resolution\n        },\n        'predictions': predictions,\n        'summary_stats': {\n            'class_distribution': dict(Counter(class_history)),\n            'average_confidence': float(np.mean(confidence_history)),\n            'confidence_std': float(np.std(confidence_history)),\n            'most_confident_frame': int(np.argmax(confidence_history) + 1),\n            'max_confidence': float(np.max(confidence_history))\n        }\n    }\n    \n    with open('/kaggle/working/detailed_predictions.json', 'w') as f:\n        json.dump(results_data, f, indent=2)\n    \n    return results_data\n\n# ========== USAGE EXAMPLE ==========\nvideo_path = \"/kaggle/input/changing-tires/hand_tighten_bolts/hand_tighten_bolts_13.mp4\"\n\n# Check if video exists\nif Path(video_path).exists():\n    print(\"🎬 Creating prediction visualization...\")\n    \n    # Create visualization video\n    results = create_prediction_video_with_plot(\n        model=loaded_streaming_model,\n        video_path=video_path,\n        class_names=class_names_list,\n        resolution=resolution,\n        max_frames=50,  # Adjust based on your needs\n        output_path='/kaggle/working/prediction_visualization.mp4'\n    )\n    \n    if results:\n        print(f\"\\n📊 FINAL SUMMARY:\")\n        print(f\"Total frames processed: {results['video_info']['total_frames_processed']}\")\n        print(f\"Average confidence: {results['summary_stats']['average_confidence']:.3f}\")\n        print(f\"Most confident prediction: Frame {results['summary_stats']['most_confident_frame']} ({results['summary_stats']['max_confidence']:.3f})\")\n        \n        print(f\"\\n📈 Class Distribution:\")\n        for class_name, count in results['summary_stats']['class_distribution'].items():\n            percentage = (count / results['video_info']['total_frames_processed']) * 100\n            print(f\"  {class_name}: {count} frames ({percentage:.1f}%)\")\n        \n        print(f\"\\n💾 Files saved:\")\n        print(f\"  - Visualization video: /kaggle/working/prediction_visualization.mp4\")\n        print(f\"  - Detailed results: /kaggle/working/detailed_predictions.json\")\n        \nelse:\n    print(f\"❌ Video file not found: {video_path}\")\n\n# ========== SIMPLE PLOT GENERATION (Alternative) ==========\ndef create_simple_confidence_plot(predictions, output_path='/kaggle/working/confidence_plot.png'):\n    \"\"\"Create a simple static plot of confidence over time\"\"\"\n    \n    frames = [p['frame'] for p in predictions]\n    confidences = [p['confidence'] for p in predictions]\n    class_names_frames = [p['class_name'] for p in predictions]\n    \n    plt.figure(figsize=(12, 8))\n    \n    # Main confidence plot\n    plt.subplot(2, 1, 1)\n    plt.plot(frames, confidences, 'b-', linewidth=2, alpha=0.7)\n    plt.scatter(frames, confidences, c=[hash(name) for name in class_names_frames], \n                cmap='tab10', s=50, alpha=0.8)\n    \n    # Moving average\n    window = 5\n    if len(confidences) >= window:\n        moving_avg = []\n        for i in range(window-1, len(confidences)):\n            avg = np.mean(confidences[i-window+1:i+1])\n            moving_avg.append(avg)\n        plt.plot(frames[window-1:], moving_avg, 'r--', linewidth=2, label=f'Moving Average ({window})')\n        plt.legend()\n    \n    plt.xlabel('Frame Number')\n    plt.ylabel('Confidence')\n    plt.title('Prediction Confidence Over Time')\n    plt.grid(True, alpha=0.3)\n    plt.ylim(0, 1)\n    \n    # Class distribution\n    plt.subplot(2, 1, 2)\n    class_counts = Counter(class_names_frames)\n    classes = list(class_counts.keys())\n    counts = list(class_counts.values())\n    \n    bars = plt.bar(classes, counts, alpha=0.7)\n    plt.xlabel('Classes')\n    plt.ylabel('Frame Count')\n    plt.title('Class Distribution')\n    plt.xticks(rotation=45)\n    \n    # Add count labels\n    for bar, count in zip(bars, counts):\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n                f'{count}', ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    print(f\"📊 Confidence plot saved to: {output_path}\")\n\n# Uncomment to create simple static plot instead of video\n# if 'results' in locals() and results:\n#     create_simple_confidence_plot(results['predictions'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T16:12:48.696089Z","iopub.execute_input":"2025-06-05T16:12:48.696385Z","iopub.status.idle":"2025-06-05T16:28:30.797807Z","shell.execute_reply.started":"2025-06-05T16:12:48.696364Z","shell.execute_reply":"2025-06-05T16:28:30.797116Z"}},"outputs":[{"name":"stdout","text":"🎬 Creating prediction visualization...\n\nCreating prediction visualization for: /kaggle/input/changing-tires/hand_tighten_bolts/hand_tighten_bolts_13.mp4\nStates initialized successfully.\nProcessing 50 frames at 30 FPS...\nProcessing frames for predictions...\nProcessed frame 10/50\nProcessed frame 20/50\nProcessed frame 30/50\nProcessed frame 40/50\nProcessed frame 50/50\nGenerating visualization video with 50 frames...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/20757013.py:194: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n","output_type":"stream"},{"name":"stdout","text":"Generated frame 20/50 of visualization\nGenerated frame 40/50 of visualization\n✅ Visualization video saved to: /kaggle/working/prediction_visualization.mp4\n\n📊 FINAL SUMMARY:\nTotal frames processed: 50\nAverage confidence: 0.802\nMost confident prediction: Frame 45 (0.989)\n\n📈 Class Distribution:\n  lower_car: 2 frames (4.0%)\n  place_spare_tire: 4 frames (8.0%)\n  hand_tighten_bolts: 44 frames (88.0%)\n\n💾 Files saved:\n  - Visualization video: /kaggle/working/prediction_visualization.mp4\n  - Detailed results: /kaggle/working/detailed_predictions.json\n","output_type":"stream"}],"execution_count":23}]}